{
 "metadata": {
  "name": "C++ Thrust - Click to Open",
  "signature": "sha256:23b2b3a5af0d95f81ee9a544ebbf7f57b9f06c437a9a145b69799aa85cc982ad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Using Thrust to Accelerate C++\n\nIn this self-paced, hands-on lab, we will learn how to use the Thrust parallel programming library to accelerate C++ code on GPUs and CPUs.\n\nLab created by Mark Ebersole (Follow [@CUDAHamster](https://twitter.com/@cudahamster) on Twitter), with examples and text borrowed heavily from the [Thrust website](http://thrust.github.io/) created by Jared Hoberock."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<script src=\"files/countdown_v5.0/countdown.js\"></script>\n<div id=\"clock\" align=\"center\"></div>\n<script>\nmyDate = new Date();\ncurTime = Date.UTC(myDate.getUTCFullYear(), \n                   myDate.getUTCMonth(), \n                   myDate.getUTCDate(), \n                   myDate.getUTCHours(), \n                   myDate.getUTCMinutes(),\n                   myDate.getUTCSeconds(),\n                   myDate.getUTCMilliseconds());\n\nfunction countdownComplete(){\n  \talert(\"You only have five minutes left in the lab! Time to save your work - see the Post Lab section near the bottom.\");\n}\nvar myCD = new Countdown({\n                         time  \t: (1486518717891+110*60000-curTime)/1000,\n                         target\t \t: \"clock\",\n                         onComplete\t: countdownComplete,\n                         rangeHi  : \"minute\",\n                         hideLine\t: true,\n                         hideLabels\t: false,\n                         height\t \t: 60,\n                         width     : 150,\n                         style     : \"boring\",\n                    });\n </script>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "---\nBefore we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see get some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print \"The answer should be three: \" + str(1+2)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "The answer should be three: 3\n"
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let's execute the cell below to display information about the GPUs running on the server."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "!nvidia-smi",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Wed Feb  8 01:57:39 2017       \r\n+------------------------------------------------------+                       \r\n| NVIDIA-SMI 340.29     Driver Version: 340.29         |                       \r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GRID K520           On   | 0000:00:03.0     Off |                  N/A |\r\n| N/A   34C    P8    17W / 125W |     10MiB /  4095MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Compute processes:                                               GPU Memory |\r\n|  GPU       PID  Process name                                     Usage      |\r\n|=============================================================================|\r\n|  No running compute processes found                                         |\r\n+-----------------------------------------------------------------------------+\r\n"
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "---\n<p class=\"hint_trigger\">If you have never before taken an IPython Notebook based self-paced lab from NVIDIA, click this green box.\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The following video will explain the infrastructure we are using for this self-paced lab, as well as give some tips on it's usage.  If you've never taken a lab on this sytem before, it's highly encourage you watch this short video first.<br><br>\n<div align=\"center\"><iframe width=\"640\" height=\"390\" src=\"http://www.youtube.com/embed/ZMrDaLSFqpY\" frameborder=\"0\" allowfullscreen></iframe></div>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Introduction to Thrust\n\nThrust is a parallel algorithms library loosely based on the C++ Standard Template Library. Thrust provides a number of building blocks, such as sort, scans, transforms, and reductions, to enable developers to quickly embrace the power of parallel computing.  In addition to targeting the massive parallelism of NVIDIA GPUs, Thrust supports multiple system back-ends such as OpenMP and Intel\u2019s Threading Building Blocks. This means that it\u2019s possible to compile your code for different parallel processors with a simple flick of a compiler switch.\n\nThis lab consists of four tasks that will require you to modify code, compile and execute it.  For each task, a solution is provided so you can check your work or take a peek if you get lost.  It is expected to take a C++ programmer new to Thrust about 90 minutes to complete the tasks in his lab.  If you are confused at any point in this lab, you can consult the <a href=\"#FAQ\">FAQ</a> located at the bottom of this page."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Task #1\n\nThe goal of your first task is to get you warmed up writing code in Thrust.  If you have ever used the C++ Standard Template Library (STL) before, this task should be fairly straight forward.  The code in Task #1 will move some randomly generated data to the GPU, sort it, and then copy it back to the host.  Before we jump into the code, let's go over the basics of working with Thrust.  \n\nFirst off, to help avoid naming conflicts, C++ makes use of namepsaces and Thrust is no exception.  In this lab and in most of the Thrust code you will see, all the Thrust functions and members will be preceded by `thrust::` to indicate which namespace it comes from.  In this this lab, you will also see reference to the `std::` namespace for printing out values to the screen.\n\n#### Containers\n\nWhereas the STL has many different types of containers, Thrust just works with two vector types:\n\n* Host vectors are declared with `thrust::host_vector<type>`\n* Device vectors are declared with `thrust::device_vector<type>`\n\nWhen declaring a host or device vector, you must provide the data type it will contain.  In fact, since Thrust is a template library, most of your declarations will involve specifying a type.  These types can be common simple native data-types like `int`, `char`, or `float`.  But the type can also be complex structures like a `thrust::tuple` which contains multiple elements.  For details on how to initialize a host or device vector, I encourage you to look at the Thrust documentation [here](http://thrust.github.io/doc/group__container__classes.html).  For this lab, the two methods needed to initialize a Thrust vector are the following:\n\n* Create a host or device vector of a specific size:  `thrust::host_vector<type> h_vec( SIZE );` or `thrust::device_vector<type> d_vec( SIZE );`\n * It's common practice to proceed host vector variables with `h_` and device vector variables with `d_` to make it clear in the code which memory space they are referring to.\n* Create and initialize a device vector from an existing Thrust vector: `thrust::device_vector<type> d_vec = h_vec;`\n * Under the covers, Thrust will handle allocating space on the device that is the same size as `h_vec`, as well as copying the memory from the host to the device.\n \n#### Interators\n\nNow that we have containers for our data in Thrust, we need a way for our algorithms to access this data, regardless of what type of data they contain.  This is where C++ iterators come in to play.  In the case of vector containers, which are really just arrays, iterators can be thought of as pointers to array elements. Therefore, `H.begin()` is an iterator that points to the first element of the array stored inside the H vector. Similarly, `H.end()` points to the element one past the last element of the H vector.\n\nAlthough vector iterators are similar to pointers, they carry more information with them. We do not have to tell Thrust algorithms that they are operating on a device_vector or host_vector iterator. This information is captured in the type of the iterator returned by `H.begin()`. When a Thrust function is called, it inspects the type of the iterator to determine whether to use a host or a device implementation. This process is known as static dispatching since the host/device dispatch is resolved at compile time. Note that this implies that there is **no runtime overhead** to the dispatch process.\n\n#### Functions\n\nWith containers and iterators, we can finally process our data using functions.  Almost all Thrust functions process the data by using iterators pointing at different vectors.  For example, to copy data from a device vector to a host vector, the following code is used:\n\n`thrust::copy( d_vec.begin(), d_vec.end(), h_vec.begin() );`\n    \nThis function simply states \"Starting at the first element of `d_vec`, copy the data starting at the beginning of `h_vec`, advancing through each vector until the end of `d_vec` is reached.\"\n\n#### Task Instructions\n\nYour objective in this task is to replace the `#FIXME` of task1.cu with code that does the following:\n\n1. Create a device_vector and copy the initialized `h_vec` data to it using the `=` operator as discussed above\n2. Sort the data on the device with [`thrust::sort`](http://thrust.github.io/doc/group__sorting.html#ga01621fff7b6eb24fb68944cc2f10af6a)\n3. Move the data back to `h_vec` using [`thrust::copy`](http://thrust.github.io/doc/group__copying.html#ga24ccfaaa706a9163ec5117758fdb71b9)\n\nThe solution to this task is provided in task1_solution.cu in the editor below.  Please look at it to check your work, or if you get stuck.  You can find this file by clicking on the \"task1\" folder on the left of the text editor, then selecting task1_solution.cu.\n\nAfter making a change, **make sure to save the file** by simply clicking the `save` button below.  As a reminder, saving the file actually saves it on the Amazon GPU system in the cloud you're running on.  To get a copy of the files we'll be working on, consult the <a href=\"#post-lab\">Post-Lab</a> section near the end of this page.  Also remember to keep an eye on the time.  The instance you are running on will shut down after 120 minutes from when you started the lab, so make sure to save your work before times run out!\n\nTo compile and run, simply execute the cell below the text editor.  As a reminder, you can do this by giving the cell focus with your mouse, hitting Ctrl-Enter or pressing the Play button in the toolbar at the top of the window."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task1\" src=\"task1\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task1.cu and if successful, run the program\n!nvcc -O2 -arch=sm_30 task1/task1.cu -run",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Runtime: 3.151465 s\r\n"
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Congrats!  You have successfully executed code on the GPU using Thrust, and you did not have to write any GPU specific code!  As we'll see in <a href=\"#task4\">Task #4</a>, with just a compiler switch, you can compile Thrust code to execute on a CPU."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Task #2\n\nMost of the Thrust functions are intended to be building blocks, allowing you the programmer to build complex algorithms on top of them.  The purpose of this task is to give you more experience using Thrust functions and iterators, and to expose you to additional functions available.  \n\nIn addition, you will also start working with \"functors\" in this task.  A functor is a \u201cfunction object\u201d, which is an object that can be called as if it were an ordinary function. In C++ a functor is just a class or struct that defines the function call operator. Because they are objects, functors can be passed (along with their state) to other functions as a parameter. Thrust comes with a handful of pre-defined functors, one of which we'll use in this task.  In the next task, we'll see how to write your own functor and use it in a Thrust algorithm. \n\nThere are a few ways to use a functor.  One is to create it like you would a normal object like this:\n\n    thrust::modulus<float> modulusFunctor(...); // Create the functor, if required, pass in any arguments to the constructor.\n    float result = modulusFunctor(4.0, 2.0); // Use the functor just like a regular function\n    ...\n    \nThe second method is to call the constructor directly in an argument list to another function:\n\n    thrust::transform(..., thrust::modulus<float>() );\n    \nYou'll notice we have to add the `()` after `<float>` as we're calling the functors constructor to instantiate the function object.  The Thrust `transform` function can now apply the functor to all the elements it's working with.\n\nUsing the editor below, open up task2.cu as before (click on the task2 folder, then on task2.cu).  Your objective is to replace the `#FIXME` sections of code to achieve the following.  Note that each item links to the relevant Thrust documentation.\n\n1. Initialize the `X` vector with 0,1,2,3,...,9 using [`thrust::sequence`](http://thrust.github.io/doc/group__transformations.html#ga08c1dd7914c155d5fed6458330af3443)\n2. Fill the `Z` vector with all 2's using [`thrust::fill`](http://thrust.github.io/doc/group__filling.html#ga95479e3d2f1af2ca19aedf7fd9db0465)\n3. Set `Y` equal to `X mod Z` using [`thrust::transform`](http://thrust.github.io/doc/group__transformations.html#ga68a3ba7d332887f1332ca3bc04453792) and [`thrust::modulus`](http://thrust.github.io/doc/structthrust_1_1modulus.html)\n4. Replace all the 1's in `Y` with 10's with [`thrust::replace`](http://thrust.github.io/doc/group__replacing.html#gaf4c7616600c8937aa31c73417cfb4f28)\n5. Print out the result of `Y` with [`thrust::copy`](http://thrust.github.io/doc/group__copying.html#ga24ccfaaa706a9163ec5117758fdb71b9) and copying it to the output iterator `std::ostream_iterator<int>(std::cout, \"\\n\")`\n\nTo make sure you are getting the correct answer, the program prints out the `Y` device vector.  If everything was done correctly, you should see the following output:<pre>\n0\n10\n0\n10\n0\n10\n0\n10\n0\n10\n</pre>\n\nThe cells to compile and execute the program are located below the editor.  If you get stuck, there are a number of hints provided - just click on the green box to see each hint.\n\nFinally, you can open up task2_solution.cu to check your work, or to look at if you feel lost."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<p class=\"hint_trigger\">Hint #1\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">If you use the `thrust::modulus<int>` constructor directly in the `thrust::transform` argument list, don't forget to add the `()` to call the constructor.</div></div></div></p>\n      \n<p class=\"hint_trigger\">Hint #2\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The last parameter to `thrust::copy` to print out the `Y` vector should be `std::ostream_iterator<int>(std::cout, \"\\n\")`.  This says that as each element is copied to the `std::ostream_iterator`, print it to `std::cout` and append a newline to it.</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task2\" src=\"task2\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task2.cu and if successful, run the program\n!nvcc -O2 -arch=sm_30 task2/task2.cu -run",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "0\r\n10\r\n0\r\n10\r\n0\r\n10\r\n0\r\n10\r\n0\r\n10\r\n"
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "If you are still not able to get the correct output, please have a look at the task2_solution.cu file and see if you can figure out what you were missing!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Task #3\n\nThrust provides a few built-in functors for you to use, but the real power comes from creating your own functors.  For this task, we will remove the call to `thrust::replace` from the code in Task #2, and instead replace that functionality with a custom functor used in the `thrust::transform` call.  An example of a custom functor is the following unary functor which returns the square of the input value:\n<pre>\ntemplate &lt;typename T&gt;\nstruct square\n{\n  __host__ __device__\n  T operator()(const T& x) const\n  { \n    return x * x;\n  }\n};\n</pre>\n\nThe `__host__ __device__` line above tells the nvcc compiler to compile both a Host and a Device version of the function below it.  This maintains portability between CPUs and GPUs.\n\nThe way this functor works is we're overriding the `()` operator of the struct.  This is the most versatile of the overloadable operators, as it can accept any number and type of inputs, and return any type of output.  In this way, the Thrust algorithms simply has to call `outputType = someFunctor( inputType1, inputType2, ..., inputTypeN )` without having to have an understanding of what the function does.  This makes for a very powerful and flexible library!\n\n**Note:** It is not necessary to make your custom functor a template struct, but it adds a lot of flexibility to your code.\n\nIn task3.cu below, finish creating the `modZeroOrTen` functor, and then call it from the `thrust::transform` function.  If everything is done right, you should get the same output as Task #2, which is:\n<pre>\n0\n10\n0\n10\n0\n10\n0\n10\n0\n10\n</pre>\n"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<p class=\"hint_trigger\">Hint #1\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The custom functor for our code needs to be a binary operator - it takes two values as input.  The `square` functor example shown is only a unary operator.</div></div></div></p>\n      \n<p class=\"hint_trigger\">Hint #2\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">If you are creating the `square` functor object directly in the `thrust::transform` argument list, don't forget to add the `()` to call the constructor.</div></div></div></p>\n      \n<p class=\"hint_trigger\">Hint #3\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">Don't forget to add, at a minimum, the `__device__` keyword before your function so the compiler knows to compile this function for the GPU.</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task3\" src=\"task3\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task3.cu and if successful, run the program\n!nvcc -O2 -arch=sm_30 task3/task3.cu -run ",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "0\r\n10\r\n0\r\n10\r\n0\r\n10\r\n0\r\n10\r\n0\r\n10\r\n"
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "By creating this custom functor, we were able to eliminate the `thrust::replace` call, which makes for a more efficient application."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Task #4\n\nSo far, we've only dealt with basic iterators that allow Thrust to cycle through all the elements in a vector.  Fancy iterators perform a variety of valuable purposes. In this task we'll show how fancy iterators allow us to attack a broader class of problems with the standard Thrust algorithms.  While we won't cover all the fancy iterators in this task, we'll cover three of them.\n\nThe simplest of the bunch, `constant_iterator` is simply an iterator that returns the same value whenever we dereference it. In the following example we initialize a `constant_iterator` with the value 10.\n<pre>\n// create iterators\nthrust::constant_iterator<int> first(10);\nthrust::constant_iterator<int> last = first + 3; // Set the last element to be 3 past the beginning\n\n// sum of [first, last)\nthrust::reduce(first, last);   // returns 30 (i.e. 10 + 10 + 10)\n</pre>\n\nThe transform_iterator allows us to apply the technique of combining separate algorithms, without having to rely on Thrust to provide a special transform_xxx version of the algorithm. This task shows another way to fuse a transformation with a reduction, this time with just plain reduce applied to a transform_iterator.\n\nThe following example prints out all the elements in the `values` vector, after clamping them between 0 and 100.\n<pre>\nthrust::copy(thrust::make_transform_iterator(values.begin(), clamp<int>(0, 100)), // First element\n             thrust::make_transform_iterator(values.end(), clamp<int>(0, 100)), // End element\n             std::ostream_iterator<int>(std::cout, \" \"));\n</pre>\n\nFinally, the zip_iterator is an **extremely** useful gadget: it takes multiple input sequences and yields a sequence of tuples.  The following example applies the `arbitrary_functor` to each tuple, where each tuple is made up of elements from `A`, `B`, `C`, and `D` vectors.  You can see details on the thrust::for_each function [here](http://thrust.github.io/doc/group__modifying.html#ga263741e1287daa9edbac8d56c95070ba).\n<pre>\nthrust::for_each(thrust::make_zip_iterator(thrust::make_tuple(A.begin(), B.begin(), C.begin(), D.begin())),\n                 thrust::make_zip_iterator(thrust::make_tuple(A.end(),   B.end(),   C.end(),   D.end())),\n                 arbitrary_functor());\n</pre>\n\nOne downside of transform_iterator and zip_iterator is that it can be cumbersome to specify the full type of the iterator, which can be quite lengthy. For this reason, it is common practice to simply put the call to make_transform_iterator or make_zip_iterator in the arguments of the algorithm being invoked.\n\nYour objective in this task is to modify task4.cu and write the code to implement each type of iterator.  The different iterator types are broken up into three functions - there is no need to modify the `main()` function.  If you want, you can comment out the inside of the functions you have yet to implement while you focus on one.  The goal for each is to replace the `#FIXME` sections:\n\n1. For the constant_iterator, use [`thrust::reduce`](http://thrust.github.io/doc/group__reductions.html#ga69434d74f2e6117040fb38d1a28016c2) and [`thrust::constant_iterator`](http://thrust.github.io/doc/classthrust_1_1constant__iterator.html) to print out the value 20.\n2. For the transform_iterator, use the [`thrust::make_transform_iterator`](http://thrust.github.io/doc/classthrust_1_1transform__iterator.html) and [`thrust::negate`](http://thrust.github.io/doc/structthrust_1_1negate.html) functor to print out -60.\n3. For the zip_iterator, use `thrust::make_tuple` and `thrust::make_zip_iterator` to return the maximum element of the tuple made up of (A, B)."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task4\" src=\"task4\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task4.cu and if successful, run the program\n!nvcc -O2 -arch=sm_30 -o task4_out task4/task4.cu -run",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Constant Iterator:\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "  value = 25. Should be 20\r\n\r\nTransform Iterator:\r\n  value = -60. Should be -60\r\n\r\nZip Iterator:\r\n  value = (30,z). Should be (30,z)\r\n\r\n"
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<a id=\"task4\"></a>\n### Run on the CPU\n\nThe below is an example of how to compile the solution from task #4 to execute on a multi-core CPU using Thrust's OpenMP backend.  Without modifying any code, we can change what the device vector in our Thrust code is targeting!\n\nThe `-Xcompiler` command tells nvcc to apply the following options to the host compiler, in our case, that's gcc.  **Note:** It's possible to compile Thrust code with the non-CUDA backend using a standard host compiler."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task4.cu for the CPU\n!nvcc -O2 -arch=sm_30 -o task4_cpu task4/task4_solution.cu -Xcompiler -fopenmp -DTHRUST_DEVICE_SYSTEM=THRUST_DEVICE_SYSTEM_OMP -lgomp",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "To prove that the compiled task4_cpu is indeed not running on the GPU, we can profile it with NVIDIA's command-line GPU profiler `nvprof`.  After executing the above cell to compiler the code with an OpenMP backend, execute the below cell.  You should see `Warning: No CUDA application was profiled, exiting` in the output, indicating that nothing was run on the GPU.\n\nIf you'd like to profile the GPU version, simply change `./task4_cpu` to `./task4_out` after successfully compiling the code from Task #4."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "!nvprof ./task4_cpu",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Constant Iterator:\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "  value = 25. Should be 20\r\n\r\nTransform Iterator:\r\n  value = -60. Should be -60\r\n\r\nZip Iterator:\r\n  value = (30,z). Should be (30,z)\r\n\r\n======== Warning: No CUDA application was profiled, exiting\r\n"
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Task #5\n\nFor the final task in our lab, we'll see how to handle and interpret errors from Thrust calls.  As with most programming, it's very important to check for errors in your code.\n\n#### Catching Exceptions\n\nWhen a Thrust function detects an error condition resulting from a lower-level API, it throws an exception to communicate the error to the caller. In Thrust, exceptions are usually instances of `thrust::system_error` objects, which inherits from `std::runtime_error`. You can catch these exceptions in your code and try to handle the error as appropriate.  Below is the syntax for catching general system errors in Thrust:\n\n```\n  try\n  {\n    // Some code\n  }\n  catch(thrust::system_error &e)\n  {\n    // output an error message\n    std::cerr << \"Error: \" << e.what() << std::endl;\n    \n    // either exit or handle the error condition\n  }\n```\n\nSome Thrust functions may require allocating temporary storage. If such an allocation fails, `std::bad_alloc`, instead of `thrust::system_error`, is returned.  The syntax is as follows:\n\n```\n  try\n  {\n    // Some code that allocates data\n  }\n  catch(std::bad_alloc &e)\n  {\n    std::cerr << \"Ran out of memory\" << std::endl;\n    \n    // either exit or handle the error condition  \n  }\n```\n\nYou can also check for different error types at once.\n\n```\n  try\n  {\n    // Some code\n  }\n  catch(std::bad_alloc &e)\n  {\n    std::cerr << \"Ran out of memory\" << std::endl;\n    \n    // either exit or handle the error condition  \n  }\n  catch(thrust::system_error &e)\n  {\n    // output an error message\n    std::cerr << \"Error: \" << e.what() << std::endl;\n    \n    // either exit or handle the error condition\n  }\n```\n\nIn the following code, add error some exception catching to catch errors.  You can either use the generic system error exception, or more specific ones, or both!  First try executing the code without modification to see what happens."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<p class=\"hint_trigger\">Hint #1\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">Don't forget to include the <code>&lt;thrust/system_error.h&gt;</code> file.</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task5\" src=\"task5\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task5.cu and if successful, run the program\n!nvcc -O2 -arch=sm_30 task5/task5.cu -run",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "You likely did not have the same exact error catching code as I did, but hopefully you get the idea.  Feel free to fix the errors in the code and make sure it runs to completion.  For additional information on handling exceptions in C++, you look at the many tutorials available searching for \"C++ exception handling\" in your favorite search engine.\n\n#### Asynchronous Error Detection\n\nFor performance, many Thrust functions are asynchronous, which means the execution of the function may be deferred until later. A consequence is that errors from asynchronous functions may be reported later in the program's execution than expected. This can make it difficult to pinpoint the source of the error.\n\nThe code below has an asynchronous error in it.  You can see what happens by executing the cell below the code.\n\n```\n#include <thrust/transform.h>\n#include <thrust/device_vector.h>\n#include <thrust/device_ptr.h>\n#include <thrust/functional.h>\n#include <iostream>\n\nint main(void)\n{\n  thrust::device_vector<int> x(1000);\n\n  std::cerr << \"Before transform.\" << std::endl;\n\n  // transform into a bogus location\n  thrust::transform(x.begin(), x.end(), thrust::device_pointer_cast<int>(0), thrust::negate<int>());\n\n  std::cerr << \"After transform.\" << std::endl;\n\n  return 0;\n}\n```"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task6.cu and if successful, run the program\n!nvcc -O2 -arch=sm_30 task6/task6.cu -run",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Before transform.\r\nAfter transform.\r\nterminate called after throwing an instance of 'thrust::system::system_error'\r\n  what():  an illegal memory access was encountered\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Aborted (core dumped)\r\n"
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "You should seen something like the below:\n\n<pre>Before transform.\nAfter transform.\nterminate called after throwing an instance of 'thrust::system::system_error'\n  what():  an illegal memory access was encountered\nAborted (core dumped)</pre>\n\nWhat's happening is the thrust::transform call is asynchronous, so execution continues immediately after calling the function.  This means we see the \"After transform\" text before the error occurs.  While debugging, we can make Thrust check for errors at the earliest opportunity by enabling a debug mode by simply defining the macro THRUST_DEBUG on the compiler's command line. This makes it easy to find the source of the error.  Try executing the below line to enable Thrust debugging."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task6.cu and if successful, run the program\n!nvcc -DTHRUST_DEBUG -O2 -arch=sm_30 task6/task6.cu -run",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Before transform.\r\nterminate called after throwing an instance of 'thrust::system::system_error'\r\n  what():  synchronize: launch_closure_by_value: an illegal memory access was encountered\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Aborted (core dumped)\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Before transform.\r\nterminate called after throwing an instance of 'thrust::system::system_error'\r\n  what():  synchronize: launch_closure_by_value: an illegal memory access was encountered\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Aborted (core dumped)\r\n"
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Case Study\n\nThe final exercise in this lab is to implement everything you've learned in the following Case Study called \"Fun with Points.\"  Your task is to modify the application to move code from the CPU to the GPU using Thrust.  The outline of what the application does is described in the image below.\n\n<div align=\"center\"><img src=\"files/fig1.PNG\" alt=\"Fun With Points\" style=\"width: 60%;\"/></div>\n\nModify the task7.cu file in the editor below to move functions to the GPU using Thrust.  The locations were you need to use Thrust are proceeded with text that has \"TODO\" at the front. As usual, if you get stuck, you can look at the task7_solution.cu file to see the correct answer.\n\nYou can find more documentation about this example at [its GitHub location](https://github.com/jaredhoberock/thrust-workshop/tree/master/fun_with_points)."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task7\" src=\"task7\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task7.cu and if successful, run the program\n!nvcc -O2 -arch=sm_30 task7/task7.cu -run",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Learn More\n\nIf you are interested in learning more, you can use the following resources:\n\n* Learn more at the [CUDA Developer Zone](https://developer.nvidia.com/Thrust).\n* Thrust is provided with the [CUDA tookit](https://developer.nvidia.com/cuda-toolkit), or you can download separately from the Thrust GitHub [site](https://github.com/thrust/thrust).\n* Look for recorded sessions on Thrust at [gputechconf.com](http://www.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=thrust&searchItems=&sessionTopic=&sessionEvent=&sessionYear=&sessionFormat=&submit=&select=+).  These include talks specifically about Thrust, or projects which made use of the library.\n* Search or ask questions on [Stackoverflow](http://stackoverflow.com/questions/tagged/thrust) using the thrust tag"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<a id=\"post-lab\"></a>\n## Post-Lab\n\nFinally, don't forget to save your work from this lab before time runs out and the instance shuts down!!\n\n1. Save this IPython Notebook by going to `File -> Download as -> IPython (.ipynb)` at the top of this window.  Note that when running this IPython Notebook locally, the browser-based text editors will not be functioning.  You will need to load the source files in your own local IDE.\n2. You can execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "%%bash\nrm -f thrust_files.zip\nzip -r thrust_files.zip task*/*",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "  adding: task1/task1.cu (deflated 56%)\n  adding: task1/task1_solution.cu (deflated 58%)\n  adding: task1/timer.h (deflated 52%)\n  adding: task2/task2.cu (deflated 60%)\n  adding: task2/task2_solution.cu (deflated 60%)\n  adding: task3/task3.cu (deflated 57%)\n  adding: task3/task3_solution.cu (deflated 57%)\n  adding: task4/task4.cu (deflated 68%)\n  adding: task4/task4_solution.cu (deflated 69%)\n  adding: task5/task5.cu (deflated 62%)\n  adding: task5/task5_solution.cu (deflated 62%)\n  adding: task6/task6.cu (deflated 53%)\n  adding: task7/task7.cu (deflated 71%)\n  adding: task7/task7_solution.cu (deflated 70%)\n"
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**After** executing the above cell, you should be able to download the zip file [here](files/thrust_files.zip)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<a id=\"FAQ\"></a>\n---\n# Lab FAQ\n\nQ: I'm encountering issues executing the cells, or other technical problems?<br>\nA: Please see [this](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) infrastructure FAQ."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<style>\np.hint_trigger{\n  margin-bottom:7px;\n  margin-top:-5px;\n  background:#64E84D;\n}\n.toggle_container{\n  margin-bottom:0px;\n}\n.toggle_container p{\n  margin:2px;\n}\n.toggle_container{\n  background:#f0f0f0;\n  clear: both;\n  font-size:100%;\n}\n</style>\n<script>\n$(\"p.hint_trigger\").click(function(){\n   $(this).toggleClass(\"active\").next().slideToggle(\"normal\");\n});\n   \n$(\".toggle_container\").hide();\n</script>"
    }
   ],
   "metadata": {}
  }
 ]
}